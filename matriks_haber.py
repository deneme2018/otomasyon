import re
import csv
import pandas as pd
import requests
from bs4 import BeautifulSoup

# --- Ayarlar ---
# Matriks haberlerinin listelendiÄŸi sayfanÄ±n URL'sini buraya yazÄ±n.
MATRIKS_HABER_URL = "https://www.matriksdata.com/website/matriks-haberler" # Ã–rnek URL.
CSV_FILENAME = "matriks_haber_arsivi.csv"
ID_DOSYA = 'kayitli_haber_idleri.txt'

def id_kontrol_dosyasini_yukle(id_dosya_adi):
    """Daha Ã¶nce kaydedilmiÅŸ haber ID'lerini dosyadan yÃ¼kler."""
    try:
        with open(id_dosya_adi, 'r', encoding='utf-8') as f:
            return set(f.read().splitlines())
    except FileNotFoundError:
        return set()

def id_kontrol_dosyasini_kaydet(id_dosya_adi, kayitli_idler):
    """GÃ¼ncel haber ID'leri kÃ¼mesini dosyaya kaydeder."""
    with open(id_dosya_adi, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sorted(list(kayitli_idler))))

def temiz_konu_olustur(konu_ham):
    """
    CSV uyumluluÄŸu iÃ§in Konu metnini temizler:
    1. Yeni satÄ±r karakterlerini kaldÄ±rÄ±r.
    2. Birden fazla boÅŸluÄŸu tek bir boÅŸluÄŸa indirir.
    3. CSV ayÄ±rÄ±cÄ±sÄ± olan noktalÄ± virgÃ¼lleri (;) virgÃ¼le Ã§evirir.
    """
    # 1. Yeni satÄ±r ve sekme karakterlerini boÅŸlukla deÄŸiÅŸtir.
    konu = konu_ham.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    # 2. Birden fazla boÅŸluÄŸu tek bir boÅŸluÄŸa indir.
    konu = re.sub(r'\s+', ' ', konu).strip() 

    # 3. CSV ayÄ±rÄ±cÄ±sÄ± Ã§akÄ±ÅŸmasÄ±nÄ± engellemek iÃ§in tÃ¼m noktalÄ± virgÃ¼lleri virgÃ¼le Ã§evir.
    konu = konu.replace(';', ',') 

    # 4. CSV'yi bozabilecek Ã§ift tÄ±rnak iÅŸaretlerini (') tek tÄ±rnakla deÄŸiÅŸtir.
    konu = konu.replace('"', "'") 
    
    return konu

def haberleri_ayristir_ve_kaydet():
    print(f"ğŸ”„ Matriks haberleri {MATRIKS_HABER_URL} adresinden Ã§ekiliyor...")
    
    # 1. HTML Ä°Ã§eriÄŸini Ã‡ek
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        response = requests.get(MATRIKS_HABER_URL, headers=headers, timeout=10)
        response.raise_for_status()
        html_icerik = response.text
        print("âœ… HTML iÃ§eriÄŸi baÅŸarÄ±yla Ã§ekildi.")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Hata: URL'ye eriÅŸilemedi veya zaman aÅŸÄ±mÄ±: {e}")
        print("URL'yi kontrol edin. Sayfa oturum aÃ§ma (login) gerektiriyorsa 'Selenium' yÃ¶ntemine geÃ§meniz gerekebilir.")
        return

    # 2. KayÄ±tlÄ± ID'leri yÃ¼kle
    kayitli_idler = id_kontrol_dosyasini_yukle(ID_DOSYA)
    print(f"YÃ¼klenen Toplam KayÄ±tlÄ± ID: {len(kayitli_idler)}")

    # 3. HTML'i ayrÄ±ÅŸtÄ±r ve haberleri filtrele
    soup = BeautifulSoup(html_icerik, 'html.parser')
    # Haber tablosu satÄ±rlarÄ±nÄ± arÄ±yoruz (style="cursor:pointer;" olanlar)
    haber_satirlari = soup.find_all('tr', style=re.compile("cursor:pointer;"))

    data = []
    id_deseni = re.compile(r'/(\d+)-')

    for satir in haber_satirlari:
        # URL'yi (linki) onclick Ã¶zelliÄŸinden al
        onclick_metni = satir.get('onclick', '')
        url_match = re.search(r"document\.location='([^']+)'", onclick_metni)
        
        if not url_match:
            continue
        
        haber_url = url_match.group(1)
        
        # URL'den benzersiz ID'yi al
        id_match = id_deseni.search(haber_url)
        haber_id = id_match.group(1) if id_match else haber_url

        # Tekrarlanan KontrolÃ¼
        if haber_id in kayitli_idler:
            continue
        
        # Yeni haber! Verileri topla.
        sutunlar = satir.find_all('td')
        if len(sutunlar) >= 3:
            tarih = sutunlar[0].text.strip()
            saat = sutunlar[1].text.strip()
            konu_ham = sutunlar[2].text.strip()
            
            # Konu metnini temizleme fonksiyonu ile CSV'ye hazÄ±r hale getir.
            konu = temiz_konu_olustur(konu_ham) 
            
            data.append({
                'Tarih': tarih,
                'Saat': saat,
                'Konu': konu,
                'URL': haber_url
            })
            
            # Yeni ID'yi kayitli_idler kÃ¼mesine ekle
            kayitli_idler.add(haber_id)

    # 4. SonuÃ§larÄ± Kaydet
    if data:
        try:
            # Mevcut arÅŸiv dosyasÄ±nÄ± yÃ¼kle (varsa)
            df_eski = pd.read_csv(CSV_FILENAME, sep=';', encoding='utf-8-sig')
            df_yeni = pd.DataFrame(data)
            df_final = pd.concat([df_eski, df_yeni], ignore_index=True)
        except FileNotFoundError:
            # Dosya yoksa sadece yeni veriyi kullan
            df_final = pd.DataFrame(data)
        
        # GÃ¼ncellenmiÅŸ DataFrame'i CSV olarak kaydet
        # index=False: SatÄ±r numaralarÄ±nÄ± kaydetmez
        # encoding='utf-8-sig': TÃ¼rkÃ§e karakterler (ÅŸ,Ã§,Ã¶) iÃ§in Excel uyumlu kaydeder
        # sep=';': AyÄ±rÄ±cÄ± olarak noktalÄ± virgÃ¼l kullanÄ±r
        df_final.to_csv(CSV_FILENAME, sep=';', encoding='utf-8-sig', index=False)
        id_kontrol_dosyasini_kaydet(ID_DOSYA, kayitli_idler)

        print(f"\n--- Ä°ÅLEM BAÅARILI ---")
        print(f"ArÅŸive Eklenen Yeni Haber SayÄ±sÄ±: {len(data)}")
        print(f"Toplam KaydedilmiÅŸ Benzersiz Haber: {len(kayitli_idler)}")
        print(f"Veriler '{CSV_FILENAME}' dosyasÄ±na kaydedildi.")
    else:
        print("\n--- Ä°ÅLEM TAMAMLANDI ---")
        print("Yeni haber bulunamadÄ± veya tÃ¼m haberler zaten kayÄ±tlÄ±ydÄ±.")

if __name__ == "__main__":
    haberleri_ayristir_ve_kaydet()
